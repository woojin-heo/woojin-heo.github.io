---
layout: post
title:  "[논문] YOLO논문 번역(1)"
date:   2020-01-08T14:25:52-05:00
author: woodi
categories: Deep_learning
---
### You Only Look Once: Unified, Real-Time Object Detection (통합된, 실시간 객체 탐지)

### **< 요약 >**
우리는 사물 탐지에 대한 새로운 접근법인 YOLO를 소개한다. 사물 탐지에 대한 이전의 작업들은 분류에서 디텍션으로 용도를 바꾼다. 대신, 사물 탐지를 공간적으로 분리된 바운딩 박스와 연관된 클래스 확률을 찾기 위한 회귀 문제로 구조화 한다. 하나의 뉴럴 네트워크는 바운딩 박스와 클래스 확률을 한번의 평가로 전체 사진으로부터 직접적으로 예측한다. 전체 디텍션의 파이프라인이 하나의 네트워크에 있기 때문에 이는 디텍션 성능을 처음부터 끝까지 바로 최적화 시킬 수 있다.

 우리의 통합된 아키텍쳐는 매우 빠르다. 기본적인 YOLO모델은 초당 45개의 프레임을 실시간으로 처리한다. 네트워크의 더 작은 버전인 fast YOLO는 매우 놀랍게도 초당 155개의 프레임을 처리하면서도 다른 실시간 감지 모델의 두배의 성과를 보인다. State-of-the-art디텍션 시스템과 비교해서 YOLO는 좀 더 많은 localization에러를 내지만 아무것도 없는 것에 대한 잘못된 예측은 훨씬 적게 한다. 마지막으로 YOLO는 사물의 매우 일반적인 모습을 학습한다. 이는 일반적인 이미지를 예술작품으로 변환하는데 있어서 (피카소 데이터셋과 people-art데이터 셋 모두에서)DPM과 R-CNN을 포함한 다른 모든 디텍션 방법들보다 성능이 훨씬 뛰어나다.

### **< Introduction >**
사람들은 사진을 한번 보고는 즉시 어떤 물체가 그 이미지에 있는지, 어디에 있는지 그리고 그것이 어떻게 상호작용하는지 안다. 인간의 시각 시스템은 빠르고 정확하며 우리로 하여금 운전이나 복잡한 일을 수행할 수 있게 한다. 사물 탐지를 위한 빠르고, 정확한 알고리즘은 컴퓨터로 하여금 어느 계절에서나 특별한 센서 없이 운전을 할 수 있게 하고, 보조 장치가 실시간 장면의 정보를 인간에게 전달하게 하는 등 반응형 로보틱 시스템에 대한 범용적인 가능성을 연다.

현재 디텍션 시스템은 분류에서 탐지로 목적을 바꾸고 있다. 사물을 탐지하기 위해서 이 시스템들은 분류기를 테스트 이미지에 여러 위치로, 여러 사이즈로 넣어서 평가한다. DPM(deformable parts models)과 같은 시스템은 분류기가 전체 이미지의 모든 위치에서 조금씩 움직이면서 탐지하는 슬라이딩 윈도우 방식을 사용한다.

R-CNN과 같은 더 최근의 접근법은 region proposal methods를 사용해 첫번째로, 바운딩 박스를 생성하고 다음에 그 박스들에 대해 분류기를 돌린다. 분류를 하고 나서는 바운딩 박스를 다듬기 위한 후 처리를 하는데, 중복된 탐지를 제거하고 장면의 다른 사물들을 기준으로 박스들에 대해 점수를 다시 매긴다. 이러한 복잡한 파이프라인은 느리고 최적화하기가 힘들다. 왜냐하면 개별적인 요소들이 각각 트레이닝 되어야 하기 때문이다.

우리는 사물 탐지를, 이미지의 픽셀에서 바로 바운딩 박스 좌표와 class 확률로 가는 단일 회귀 문제로 재구성한다. 이미지에 어떤 물건이 있고 어디에 있는지 예측하기 위해 당신은 우리의 시스템을 사용해서 딱 한번만 보면 된다. (YOLO)

![yolo_figure1](https://user-images.githubusercontent.com/55940348/72218034-8eb89d80-3579-11ea-9730-53a9af9e46ad.PNG)
![figure](./img/yolo_figure1.png)
그림1. YOLO의 탐지 시스템YOLO에서 이미지를 처리하는 법은 간단하고 직관적이다. 우리 시스템은 (1) 인풋 이미지를 448 X 448로 리사이즈한다, (2) 하나의 합성곱 네트워크를 이미지에 적용한다. 그리고 (3)임계치를 기준으로 confidence score에 대해 비최대값 억제를 한다.

 YOLO는 상쾌하게 간단하다. 사진1을 보라. 하나의 합성곱 네트워크가 동시에 여러 개의 바운딩박스들과 그 박스들의 분류 카테고리 확률을 예측한다. YOLO는 전체 이미지로부터 학습을 하고 즉시 탐지 성능을 최적화 한다.  이러한 통일된 모델은 전통적 사물 탐지 모델에 비해 몇가지의 장점을 가진다.

 첫째로, YOLO는 매우 빠르다. 우리가 탐지를 회귀 문제로 재구성했기 때문에 복잡한 루트가 필요 없다. 사물 탐지를 위한 테스트를 할 때 새로운 이미지를 우리의 신경망에서 간단히 실행하기만 하면 된다. 우리의 기본적인 네트워크는 Titan X GPU에서의 배치 과정 없이 초당 45프레임으로 작동하고 빠른 버전은 150fps를 넘어선다. 이것은 대기시간 25밀리초만으로 실시간 스트리밍 비디오 처리가 가능함을 의미한다. 게다가, YOLO는 다른 실시간 시스템보다 2배 이상의 평균 정밀도를 보인다. 실시간 웹 캠에서 작동하는 우리 시스템의 데모 영상을 이 익명의 유튜브 채널에서 참고할 수 있다. : https://goo.gl/bEs6Cj.

 두번째로, YOLO는 예측을 할 때 이미지 전체로부터 추리한다. 슬라이딩 윈도우와 region-proposal 기반의 기법들과는 다르게, YOLO는 학습과 테스트를 하는 동안 전체 이미지를 보기 때문에 클래스의 모양뿐 만 아니라 상황 정보도 encode한다. 최고의 디텍션 방법인 Fast R-CNN 조차도 사진에 넓게 있는 상황정보를 보지 못하기 때문에 배경에 있는 반점을 물체로 잘못 인식하곤 한다. YOLO에서는 이와 같은 background error가 Fast R-CNN의 절반 이하의 수준이다.

 세번째로, YOLO는 사물의 일반적인 형태를 학습한다. 보통의 이미지를 학습해서 예술작품으로 출력하는 테스트를 해보면 YOLO는 최고의 디텍션 방식인 DPM과 R-CNN에 비해 압도적으로 우수하다. YOLO는 일반화가능한 모델이기 때문에 다른 영역이나 기대하지 못한 입력값에서도 오작동 할 가능성이 거의 없다.

우리의 모든 트레이닝과 테스트 코드는 오픈소스이며 온라인에서 이용가능하다. 다양한 미리 학습된 모델들 또한 다운로드가 가능하다
:smile:

- - -

2020년 1월 10일 오후 9시 12분, 번역 woodi
원문 출처 : [You Only Look Once: Unified, Real-Time Object Detection](http://pjreddie.com/media/files/papers/yolo.pdf)